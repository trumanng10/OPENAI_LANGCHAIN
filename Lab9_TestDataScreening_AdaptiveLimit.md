# Lab 9 - Test Data Screening & Adaptive Limit Agent

## Learning Objectives

By the end of this lab, you will be able to:
- **Generate** synthetic semiconductor test data for wafer sort and final test
- **Implement** adaptive test limits using statistical process control
- **Apply** machine learning classification to identify known-good die
- **Develop** skip test strategies to reduce test time
- **Build** automated alert systems for test program optimization
- **Create** adaptive limit algorithms that learn from historical data

---

## Step-by-Step Lab Guide

### Step 1: Generate Synthetic Semiconductor Test Data

```python
# Install required packages
!pip install scikit-learn pandas numpy matplotlib seaborn xgboost
!pip install emails  # For email alerts

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Packages installed successfully!")
```

```python
class SemiconductorTestDataGenerator:
    """
    Generates synthetic semiconductor test data for wafer sort and final test
    Simulates multiple test parameters, bin codes, and process variations
    """
    
    def __init__(self, seed=42):
        self.seed = seed
        np.random.seed(seed)
        
        # Define test parameters for different product types
        self.test_parameters = {
            'digital_processor': {
                'frequency_mhz': (800, 2200),
                'leakage_current_ua': (0.1, 50),
                'vdd_min_v': (0.8, 1.2),
                'iddq_ma': (5, 100),
                'transition_time_ns': (0.1, 2.0),
                'io_voltage_v': (1.8, 3.3)
            },
            'memory_chip': {
                'access_time_ns': (10, 50),
                'standby_current_ua': (1, 100),
                'refresh_current_ma': (2, 20),
                'write_time_ns': (15, 60),
                'retention_time_ms': (1, 100)
            },
            'analog_mixed_signal': {
                'gain_db': (20, 60),
                'noise_uv': (1, 50),
                'bandwidth_mhz': (1, 100),
                'offset_mv': (-10, 10),
                'psrr_db': (40, 80)
            }
        }
    
    def generate_wafer_data(self, wafer_count=10, dies_per_wafer=500, product_type='digital_processor'):
        """Generate synthetic wafer test data with spatial correlation"""
        wafer_data = []
        
        for wafer_id in range(1, wafer_count + 1):
            wafer_lot = f"LOT{np.random.randint(1000, 9999)}"
            wafer_number = f"W{wafer_id:03d}"
            
            # Generate wafer-level process variations
            wafer_center_x, wafer_center_y = 75, 75  # Wafer center coordinates
            wafer_radius = 70
            
            # Wafer-level systematic variations
            wafer_center_offset_x = np.random.normal(0, 5)
            wafer_center_offset_y = np.random.normal(0, 5)
            wafer_rotation = np.random.normal(0, 10)  # degrees
            
            for die_id in range(1, dies_per_wafer + 1):
                # Generate die coordinates with some randomness
                angle = np.random.uniform(0, 2 * np.pi)
                radius = np.random.uniform(0, wafer_radius)
                
                # Apply wafer-level variations
                die_x = wafer_center_x + radius * np.cos(np.radians(angle + wafer_rotation)) + wafer_center_offset_x
                die_y = wafer_center_y + radius * np.sin(np.radians(angle + wafer_rotation)) + wafer_center_offset_y
                
                # Check if die is within wafer
                distance_from_center = np.sqrt((die_x - wafer_center_x)**2 + (die_y - wafer_center_y)**2)
                if distance_from_center > wafer_radius:
                    continue  # Skip dies outside wafer
                
                # Generate test parameters based on product type
                test_params = self._generate_die_test_parameters(product_type, die_x, die_y, wafer_center_x, wafer_center_y)
                
                # Determine bin code (pass/fail and categories)
                bin_code = self._determine_bin_code(test_params, product_type)
                
                # Test time simulation (some tests take longer)
                test_time = self._generate_test_time(bin_code, product_type)
                
                wafer_data.append({
                    'lot_id': wafer_lot,
                    'wafer_id': wafer_number,
                    'die_id': f"D{die_id:04d}",
                    'die_x': die_x,
                    'die_y': die_y,
                    'bin_code': bin_code,
                    'test_time_seconds': test_time,
                    'product_type': product_type,
                    'timestamp': datetime.now() - timedelta(days=np.random.randint(0, 30)),
                    **test_params
                })
        
        return pd.DataFrame(wafer_data)
    
    def _generate_die_test_parameters(self, product_type, die_x, die_y, center_x, center_y):
        """Generate test parameters with spatial correlation"""
        params = {}
        param_ranges = self.test_parameters[product_type]
        
        # Calculate distance from center (affects parameter quality)
        distance = np.sqrt((die_x - center_x)**2 + (die_y - center_y)**2)
        normalized_distance = distance / 70  # Normalize to wafer radius
        
        for param_name, (min_val, max_val) in param_ranges.items():
            # Center dies are typically better
            center_effect = 1.0 - (normalized_distance * 0.3)  # 30% degradation at edge
            
            # Add random within-die variation
            random_variation = np.random.normal(0, 0.1)
            
            # Base value with systematic variation
            if param_name in ['leakage_current_ua', 'standby_current_ua', 'offset_mv', 'noise_uv']:
                # These parameters should be lower = better
                base_value = min_val + (max_val - min_val) * normalized_distance * 0.8
                value = max(min_val, base_value * center_effect * (1 + random_variation))
            else:
                # These parameters should be higher = better
                base_value = max_val - (max_val - min_val) * normalized_distance * 0.8
                value = min(max_val, base_value * center_effect * (1 + random_variation))
            
            params[param_name] = max(min_val, min(max_val, value))
        
        return params
    
    def _determine_bin_code(self, test_params, product_type):
        """Determine bin code based on test parameters"""
        if product_type == 'digital_processor':
            # Digital processor binning logic
            freq = test_params['frequency_mhz']
            leakage = test_params['leakage_current_ua']
            vdd = test_params['vdd_min_v']
            
            if freq >= 2000 and leakage <= 5 and vdd <= 0.9:
                return 'A1'  # Premium bin
            elif freq >= 1800 and leakage <= 10 and vdd <= 1.0:
                return 'A2'  # Standard bin
            elif freq >= 1500 and leakage <= 20 and vdd <= 1.1:
                return 'B1'  # Economy bin
            elif freq >= 1000 and leakage <= 30:
                return 'B2'  # Low power bin
            else:
                return 'F'   # Fail
            
        elif product_type == 'memory_chip':
            # Memory chip binning logic
            access_time = test_params['access_time_ns']
            standby = test_params['standby_current_ua']
            
            if access_time <= 20 and standby <= 10:
                return 'A1'  # High speed
            elif access_time <= 30 and standby <= 20:
                return 'A2'  # Standard
            elif access_time <= 40 and standby <= 50:
                return 'B1'  # Low power
            else:
                return 'F'   # Fail
                
        else:  # analog_mixed_signal
            gain = test_params['gain_db']
            noise = test_params['noise_uv']
            offset = abs(test_params['offset_mv'])
            
            if gain >= 50 and noise <= 5 and offset <= 2:
                return 'A1'
            elif gain >= 40 and noise <= 10 and offset <= 5:
                return 'A2'
            elif gain >= 30 and noise <= 20 and offset <= 8:
                return 'B1'
            else:
                return 'F'
    
    def _generate_test_time(self, bin_code, product_type):
        """Generate test time based on bin code and product type"""
        base_time = {
            'digital_processor': 45,
            'memory_chip': 30,
            'analog_mixed_signal': 60
        }[product_type]
        
        # Failed dies often take longer due to retests
        if bin_code == 'F':
            return base_time * (1 + np.random.uniform(0.2, 0.5))
        else:
            return base_time * (1 + np.random.uniform(-0.1, 0.1))

# Generate the test dataset
print("üîÑ Generating synthetic semiconductor test data...")
generator = SemiconductorTestDataGenerator()

# Generate data for different product types
digital_data = generator.generate_wafer_data(wafer_count=5, dies_per_wafer=400, product_type='digital_processor')
memory_data = generator.generate_wafer_data(wafer_count=3, dies_per_wafer=300, product_type='memory_chip')
analog_data = generator.generate_wafer_data(wafer_count=2, dies_per_wafer=200, product_type='analog_mixed_signal')

# Combine all data
test_data = pd.concat([digital_data, memory_data, analog_data], ignore_index=True)

print("‚úÖ Test dataset generated successfully!")
print(f"Dataset shape: {test_data.shape}")
print(f"Total dies: {len(test_data)}")
print(f"Product types: {test_data['product_type'].value_counts().to_dict()}")
print(f"Bin code distribution:\n{test_data['bin_code'].value_counts()}")

# Display sample data
print("\nüìä Sample test data:")
print(test_data.head(10))
```

### Step 2: Exploratory Data Analysis and Test Pattern Visualization

```python
def analyze_test_data(df):
    """Comprehensive analysis of test data"""
    print("üìà TEST DATA ANALYSIS REPORT")
    print("=" * 50)
    
    # Basic statistics
    print(f"Total wafers: {df['wafer_id'].nunique()}")
    print(f"Total lots: {df['lot_id'].nunique()}")
    print(f"Yield: {(df['bin_code'] != 'F').mean() * 100:.2f}%")
    print(f"Total test time: {df['test_time_seconds'].sum() / 3600:.2f} hours")
    
    # Yield by product type
    print("\nüìä Yield by Product Type:")
    yield_by_product = df.groupby('product_type').apply(
        lambda x: (x['bin_code'] != 'F').mean() * 100
    ).round(2)
    print(yield_by_product)
    
    # Create visualizations
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Plot 1: Yield by wafer
    wafer_yield = df.groupby('wafer_id').apply(
        lambda x: (x['bin_code'] != 'F').mean() * 100
    )
    axes[0, 0].bar(range(len(wafer_yield)), wafer_yield.values)
    axes[0, 0].set_title('Yield by Wafer')
    axes[0, 0].set_ylabel('Yield (%)')
    axes[0, 0].set_xlabel('Wafer Index')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Plot 2: Bin code distribution
    bin_counts = df['bin_code'].value_counts()
    axes[0, 1].pie(bin_counts.values, labels=bin_counts.index, autopct='%1.1f%%')
    axes[0, 1].set_title('Bin Code Distribution')
    
    # Plot 3: Test time distribution
    axes[0, 2].hist(df['test_time_seconds'], bins=50, alpha=0.7)
    axes[0, 2].set_title('Test Time Distribution')
    axes[0, 2].set_xlabel('Test Time (seconds)')
    axes[0, 2].set_ylabel('Frequency')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Plot 4: Wafer maps for each product type
    product_types = df['product_type'].unique()
    for idx, product_type in enumerate(product_types):
        product_data = df[df['product_type'] == product_type]
        
        # Sample one wafer for mapping
        sample_wafer = product_data['wafer_id'].iloc[0]
        wafer_data = product_data[product_data['wafer_id'] == sample_wafer]
        
        scatter = axes[1, idx].scatter(
            wafer_data['die_x'], 
            wafer_data['die_y'], 
            c=wafer_data['bin_code'] != 'F',  # Green = pass, Red = fail
            cmap='RdYlGn',
            s=20,
            alpha=0.7
        )
        axes[1, idx].set_title(f'Wafer Map: {product_type}\n{sample_wafer}')
        axes[1, idx].set_xlabel('X Position')
        axes[1, idx].set_ylabel('Y Position')
        axes[1, idx].set_aspect('equal')
        axes[1, idx].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return fig

print("üîç Analyzing test data patterns...")
analysis_fig = analyze_test_data(test_data)

# Parameter distribution analysis
def analyze_test_parameters(df):
    """Analyze distributions of test parameters"""
    product_types = df['product_type'].unique()
    
    for product_type in product_types:
        print(f"\nüîß {product_type.upper()} - Test Parameter Analysis:")
        product_data = df[df['product_type'] == product_type]
        
        # Get test parameters for this product type
        test_params = [col for col in product_data.columns if col in generator.test_parameters[product_type]]
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for idx, param in enumerate(test_params[:6]):  # Show first 6 parameters
            if idx >= len(axes):
                break
                
            # Plot distribution by bin code
            for bin_code in ['A1', 'A2', 'B1', 'B2', 'F']:
                bin_data = product_data[product_data['bin_code'] == bin_code][param]
                if len(bin_data) > 0:
                    axes[idx].hist(bin_data, bins=30, alpha=0.6, label=f'Bin {bin_code}')
            
            axes[idx].set_title(f'{param} Distribution')
            axes[idx].set_xlabel(param)
            axes[idx].set_ylabel('Frequency')
            axes[idx].legend()
            axes[idx].grid(True, alpha=0.3)
        
        # Remove empty subplots
        for idx in range(len(test_params), len(axes)):
            fig.delaxes(axes[idx])
        
        plt.tight_layout()
        plt.show()

analyze_test_parameters(test_data)
```

### Step 3: Adaptive Test Limits using Statistical Process Control

```python
class AdaptiveTestLimits:
    """
    Implements adaptive test limits using statistical process control
    Dynamically adjusts test limits based on historical data
    """
    
    def __init__(self, initial_sigma=3, learning_rate=0.1):
        self.initial_sigma = initial_sigma
        self.learning_rate = learning_rate
        self.limit_history = {}
        self.control_limits = {}
        
    def calculate_initial_limits(self, df, product_type):
        """Calculate initial test limits from historical data"""
        product_data = df[df['product_type'] == product_type]
        passing_data = product_data[product_data['bin_code'] != 'F']
        
        test_params = [col for col in product_data.columns 
                      if col in generator.test_parameters[product_type]]
        
        limits = {}
        for param in test_params:
            param_data = passing_data[param]
            
            mean = param_data.mean()
            std = param_data.std()
            
            # Determine if parameter should be minimized or maximized
            if any(keyword in param for keyword in ['leakage', 'current', 'noise', 'offset', 'time']):
                # Lower is better
                ucl = mean + self.initial_sigma * std
                lcl = mean - self.initial_sigma * std
                optimal_direction = 'minimize'
            else:
                # Higher is better (frequency, gain, etc.)
                ucl = mean + self.initial_sigma * std
                lcl = mean - self.initial_sigma * std
                optimal_direction = 'maximize'
            
            limits[param] = {
                'mean': mean,
                'std': std,
                'ucl': ucl,
                'lcl': lcl,
                'optimal_direction': optimal_direction,
                'sample_size': len(param_data)
            }
        
        self.control_limits[product_type] = limits
        return limits
    
    def update_limits(self, new_data, product_type):
        """Update test limits with new production data"""
        if product_type not in self.control_limits:
            self.calculate_initial_limits(new_data, product_type)
            return
        
        current_limits = self.control_limits[product_type]
        passing_data = new_data[new_data['bin_code'] != 'F']
        
        updated_limits = {}
        for param, old_limits in current_limits.items():
            if param not in passing_data.columns:
                continue
                
            new_param_data = passing_data[param]
            if len(new_param_data) == 0:
                updated_limits[param] = old_limits
                continue
            
            # Calculate new statistics
            new_mean = new_param_data.mean()
            new_std = new_param_data.std()
            
            # Exponential moving average update
            alpha = self.learning_rate
            updated_mean = alpha * new_mean + (1 - alpha) * old_limits['mean']
            updated_std = alpha * new_std + (1 - alpha) * old_limits['std']
            
            # Update control limits
            if old_limits['optimal_direction'] == 'minimize':
                updated_ucl = updated_mean + self.initial_sigma * updated_std
                updated_lcl = updated_mean - self.initial_sigma * updated_std
            else:
                updated_ucl = updated_mean + self.initial_sigma * updated_std
                updated_lcl = updated_mean - self.initial_sigma * updated_std
            
            updated_limits[param] = {
                'mean': updated_mean,
                'std': updated_std,
                'ucl': updated_ucl,
                'lcl': updated_lcl,
                'optimal_direction': old_limits['optimal_direction'],
                'sample_size': old_limits['sample_size'] + len(new_param_data)
            }
        
        # Store history
        if product_type not in self.limit_history:
            self.limit_history[product_type] = []
        self.limit_history[product_type].append(current_limits)
        
        self.control_limits[product_type] = updated_limits
        return updated_limits
    
    def evaluate_test_limits(self, test_results, product_type):
        """Evaluate new test results against current limits"""
        if product_type not in self.control_limits:
            return []
        
        violations = []
        limits = self.control_limits[product_type]
        
        for param, limit_info in limits.items():
            if param not in test_results:
                continue
            
            test_value = test_results[param]
            ucl = limit_info['ucl']
            lcl = limit_info['lcl']
            
            if limit_info['optimal_direction'] == 'minimize':
                # For minimize parameters, values above UCL are bad
                if test_value > ucl:
                    violations.append({
                        'parameter': param,
                        'value': test_value,
                        'limit': ucl,
                        'direction': 'above UCL',
                        'severity': (test_value - ucl) / limit_info['std']
                    })
            else:
                # For maximize parameters, values below LCL are bad
                if test_value < lcl:
                    violations.append({
                        'parameter': param,
                        'value': test_value,
                        'limit': lcl,
                        'direction': 'below LCL', 
                        'severity': (lcl - test_value) / limit_info['std']
                    })
        
        return violations
    
    def suggest_limit_adjustments(self, product_type, violation_threshold=0.1):
        """Suggest limit adjustments based on violation patterns"""
        if product_type not in self.control_limits:
            return []
        
        suggestions = []
        limits = self.control_limits[product_type]
        
        # This would typically analyze violation patterns over time
        # For demo, we'll create some example suggestions
        for param, limit_info in limits.items():
            current_std = limit_info['std']
            
            # Suggest tightening limits if variation has reduced
            if current_std < limit_info['std'] * 0.8:  # 20% improvement
                suggestions.append({
                    'parameter': param,
                    'action': 'tighten_limits',
                    'current_std': current_std,
                    'suggested_sigma': self.initial_sigma * 0.9,  # Tighten by 10%
                    'reason': 'Reduced process variation'
                })
        
        return suggestions

# Initialize adaptive limits system
print("üéØ Initializing Adaptive Test Limits System...")
adaptive_limits = AdaptiveTestLimits(initial_sigma=3, learning_rate=0.1)

# Calculate initial limits for each product type
for product_type in test_data['product_type'].unique():
    limits = adaptive_limits.calculate_initial_limits(test_data, product_type)
    print(f"\nüìä Initial limits for {product_type}:")
    for param, limit_info in list(limits.items())[:3]:  # Show first 3
        print(f"   {param}: {limit_info['optimal_direction']}, UCL: {limit_info['ucl']:.3f}, LCL: {limit_info['lcl']:.3f}")

# Test the limit evaluation
sample_test = test_data[test_data['product_type'] == 'digital_processor'].iloc[0]
violations = adaptive_limits.evaluate_test_limits(sample_test, 'digital_processor')

print(f"\nüîç Sample test evaluation:")
print(f"Die: {sample_test['die_id']}, Bin: {sample_test['bin_code']}")
if violations:
    print("Limit violations detected:")
    for violation in violations:
        print(f"   {violation['parameter']}: {violation['value']:.3f} ({violation['direction']} {violation['limit']:.3f})")
else:
    print("No limit violations - test passed!")
```

### Step 4: Machine Learning for Known-Good Die Classification

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import xgboost as xgb

class KnownGoodDieClassifier:
    """
    Machine learning system to identify known-good die and suggest skip tests
    """
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.performance_history = {}
        
    def prepare_features(self, df, product_type):
        """Prepare features for ML classification"""
        product_data = df[df['product_type'] == product_type].copy()
        
        # Select test parameters as features
        test_params = [col for col in product_data.columns 
                      if col in generator.test_parameters[product_type]]
        
        # Create target variable (1 = known good, 0 = other)
        product_data['is_known_good'] = (
            (product_data['bin_code'].isin(['A1', 'A2'])) &  # Only premium bins
            (product_data['test_time_seconds'] < product_data['test_time_seconds'].quantile(0.8))  # Fast testing
        ).astype(int)
        
        # Spatial features
        product_data['distance_from_center'] = np.sqrt(
            (product_data['die_x'] - 75)**2 + (product_data['die_y'] - 75)**2
        )
        product_data['angle'] = np.arctan2(
            product_data['die_y'] - 75, product_data['die_x'] - 75
        )
        
        # Feature set
        features = test_params + ['distance_from_center', 'angle', 'die_x', 'die_y']
        
        X = product_data[features]
        y = product_data['is_known_good']
        
        return X, y, features, product_data
    
    def train_classifiers(self, df, product_type):
        """Train multiple classifiers for known-good die prediction"""
        X, y, features, product_data = self.prepare_features(df, product_type)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers[product_type] = scaler
        
        # Define classifiers
        classifiers = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )
        }
        
        results = {}
        
        for name, classifier in classifiers.items():
            print(f"üîÑ Training {name} for {product_type}...")
            
            # Train model
            classifier.fit(X_train_scaled, y_train)
            
            # Predictions
            y_pred = classifier.predict(X_test_scaled)
            y_pred_proba = classifier.predict_proba(X_test_scaled)[:, 1]
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision = classification_report(y_test, y_pred, output_dict=True)['1']['precision']
            recall = classification_report(y_test, y_pred, output_dict=True)['1']['recall']
            f1 = classification_report(y_test, y_pred, output_dict=True)['1']['f1-score']
            
            # Cross-validation
            cv_scores = cross_val_score(classifier, X_train_scaled, y_train, cv=5, scoring='f1')
            
            results[name] = {
                'model': classifier,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'predictions': y_pred,
                'probabilities': y_pred_proba
            }
            
            # Feature importance
            if hasattr(classifier, 'feature_importances_'):
                self.feature_importance[f"{product_type}_{name}"] = pd.DataFrame({
                    'feature': features,
                    'importance': classifier.feature_importances_
                }).sort_values('importance', ascending=False)
            
            print(f"   ‚úÖ {name} - F1: {f1:.4f}, Accuracy: {accuracy:.4f}")
        
        self.models[product_type] = results
        self.performance_history[product_type] = results
        
        return results
    
    def evaluate_classifiers(self, product_type):
        """Comprehensive evaluation of classifiers"""
        if product_type not in self.models:
            print(f"No models trained for {product_type}")
            return
        
        results = self.models[product_type]
        
        print(f"\nüìä CLASSIFIER EVALUATION - {product_type.upper()}")
        print("=" * 50)
        
        # Create comparison table
        comparison_data = []
        for name, metrics in results.items():
            comparison_data.append({
                'Classifier': name,
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1_score'],
                'CV Score': f"{metrics['cv_mean']:.4f} ¬± {metrics['cv_std']:.4f}"
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.round(4))
        
        # Visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Plot 1: Performance metrics comparison
        metrics_df = comparison_df.set_index('Classifier')[['Accuracy', 'Precision', 'Recall', 'F1-Score']]
        metrics_df.plot(kind='bar', ax=axes[0, 0])
        axes[0, 0].set_title('Classifier Performance Comparison')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # Plot 2: Feature importance (for best model)
        best_model_name = max(results.items(), key=lambda x: x[1]['f1_score'])[0]
        feature_key = f"{product_type}_{best_model_name}"
        
        if feature_key in self.feature_importance:
            top_features = self.feature_importance[feature_key].head(10)
            axes[0, 1].barh(top_features['feature'], top_features['importance'])
            axes[0, 1].set_title(f'Top 10 Features - {best_model_name}')
            axes[0, 1].set_xlabel('Importance')
        
        # Plot 3: Confusion matrix for best model
        best_model_results = results[best_model_name]
        cm = confusion_matrix(y_test, best_model_results['predictions'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],
                   xticklabels=['Not Known-Good', 'Known-Good'],
                   yticklabels=['Not Known-Good', 'Known-Good'])
        axes[1, 0].set_title(f'Confusion Matrix - {best_model_name}')
        axes[1, 0].set_ylabel('Actual')
        axes[1, 0].set_xlabel('Predicted')
        
        # Plot 4: Probability distribution
        axes[1, 1].hist(best_model_results['probabilities'], bins=50, alpha=0.7)
        axes[1, 1].set_title('Predicted Probability Distribution')
        axes[1, 1].set_xlabel('Probability of Known-Good Die')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return comparison_df
    
    def predict_known_good_dies(self, new_data, product_type, probability_threshold=0.9):
        """Predict known-good dies in new data"""
        if product_type not in self.models:
            print(f"No model trained for {product_type}")
            return None
        
        # Get best model
        best_model_name = max(self.models[product_type].items(), key=lambda x: x[1]['f1_score'])[0]
        best_model = self.models[product_type][best_model_name]['model']
        scaler = self.scalers[product_type]
        
        # Prepare features
        X, _, features, product_data = self.prepare_features(new_data, product_type)
        X_scaled = scaler.transform(X)
        
        # Predict probabilities
        probabilities = best_model.predict_proba(X_scaled)[:, 1]
        predictions = (probabilities >= probability_threshold).astype(int)
        
        # Add predictions to data
        result_data = product_data.copy()
        result_data['known_good_probability'] = probabilities
        result_data['predicted_known_good'] = predictions
        
        return result_data

# Train known-good die classifiers
print("ü§ñ Training Known-Good Die Classifiers...")
classifier = KnownGoodDieClassifier()

for product_type in test_data['product_type'].unique():
    print(f"\nüéØ Training models for {product_type}...")
    classifier.train_classifiers(test_data, product_type)
    classifier.evaluate_classifiers(product_type)

# Test prediction on new data
sample_product = 'digital_processor'
sample_data = test_data[test_data['product_type'] == sample_product].tail(100)
predictions = classifier.predict_known_good_dies(sample_data, sample_product, probability_threshold=0.85)

if predictions is not None:
    known_good_count = predictions['predicted_known_good'].sum()
    actual_known_good = predictions['is_known_good'].sum()
    
    print(f"\nüîÆ Prediction Results for {sample_product}:")
    print(f"   Predicted known-good dies: {known_good_count}")
    print(f"   Actual known-good dies: {actual_known_good}")
    print(f"   Accuracy: {(predictions['predicted_known_good'] == predictions['is_known_good']).mean() * 100:.2f}%")
    
    # Show some high-confidence predictions
    high_confidence = predictions[predictions['known_good_probability'] > 0.95].head(5)
    print(f"\nüìã High-confidence known-good dies (probability > 0.95):")
    for _, die in high_confidence.iterrows():
        print(f"   {die['die_id']}: Prob {die['known_good_probability']:.3f}, Bin {die['bin_code']}")
```

### Step 5: Skip Test Strategy Implementation

```python
class SkipTestStrategy:
    """
    Implements intelligent skip test strategies to reduce test time
    for known-good dies while maintaining quality
    """
    
    def __init__(self, test_time_reduction_target=0.3):
        self.test_time_reduction_target = test_time_reduction_target
        self.skip_rules = {}
        self.strategy_history = {}
        
    def analyze_test_patterns(self, df, product_type):
        """Analyze test patterns to identify skip opportunities"""
        product_data = df[df['product_type'] == product_type]
        
        # Get test parameters
        test_params = [col for col in product_data.columns 
                      if col in generator.test_parameters[product_type]]
        
        analysis_results = {}
        
        for param in test_params:
            param_data = product_data[param]
            
            # Calculate correlation with final bin
            param_correlation = self._calculate_parameter_importance(product_data, param)
            
            # Test time impact (simplified - in reality would have actual test times per parameter)
            time_impact = np.random.uniform(0.1, 0.5)  # Simulated test time percentage
            
            # Skip potential based on stability and correlation
            passing_std = product_data[product_data['bin_code'] != 'F'][param].std()
            overall_std = param_data.std()
            
            stability_ratio = passing_std / overall_std if overall_std > 0 else 1
            skip_potential = (1 - param_correlation) * stability_ratio
            
            analysis_results[param] = {
                'correlation_with_bin': param_correlation,
                'time_impact': time_impact,
                'stability_ratio': stability_ratio,
                'skip_potential': skip_potential,
                'passing_std': passing_std,
                'overall_std': overall_std
            }
        
        return analysis_results
    
    def _calculate_parameter_importance(self, df, parameter):
        """Calculate how important a parameter is for final bin decision"""
        # Convert bin codes to numerical (F=0, B2=1, B1=2, A2=3, A1=4)
        bin_mapping = {'F': 0, 'B2': 1, 'B1': 2, 'A2': 3, 'A1': 4}
        df['bin_numeric'] = df['bin_code'].map(bin_mapping)
        
        # Calculate correlation
        correlation = df[parameter].corr(df['bin_numeric'])
        
        return abs(correlation) if not np.isnan(correlation) else 0
    
    def generate_skip_rules(self, df, product_type, known_good_predictions=None):
        """Generate intelligent skip test rules"""
        test_analysis = self.analyze_test_patterns(df, product_type)
        
        # Sort parameters by skip potential (highest first)
        sorted_params = sorted(test_analysis.items(), 
                             key=lambda x: x[1]['skip_potential'], 
                             reverse=True)
        
        skip_rules = {}
        total_time_reduction = 0
        current_test_time = df['test_time_seconds'].mean()
        
        for param, analysis in sorted_params:
            if total_time_reduction >= self.test_time_reduction_target:
                break
            
            # Only skip parameters with high skip potential and low correlation
            if (analysis['skip_potential'] > 0.7 and 
                analysis['correlation_with_bin'] < 0.3):
                
                skip_conditions = {
                    'min_confidence': 0.9,  # Only skip for high-confidence known-good dies
                    'max_skip_rate': 0.3,   # Don't skip more than 30% of tests for this param
                    'apply_to_bins': ['A1', 'A2'],  # Only apply to premium bins
                    'time_savings': analysis['time_impact'] * current_test_time
                }
                
                skip_rules[param] = skip_conditions
                total_time_reduction += analysis['time_impact']
        
        self.skip_rules[product_type] = skip_rules
        
        print(f"\nüéØ Skip Test Rules for {product_type}:")
        print(f"   Target reduction: {self.test_time_reduction_target * 100}%")
        print(f"   Achieved reduction: {total_time_reduction * 100:.1f}%")
        print(f"   Parameters to skip: {list(skip_rules.keys())}")
        
        return skip_rules
    
    def apply_skip_strategy(self, die_data, product_type, known_good_confidence):
        """Apply skip test strategy to individual die"""
        if product_type not in self.skip_rules:
            return die_data, 0
        
        skip_rules = self.skip_rules[product_type]
        time_savings = 0
        skipped_tests = []
        
        # Only apply skip rules to high-confidence known-good dies
        if known_good_confidence >= 0.9 and die_data['bin_code'] in ['A1', 'A2']:
            for param, conditions in skip_rules.items():
                if (np.random.random() < conditions['max_skip_rate'] and 
                    known_good_confidence >= conditions['min_confidence']):
                    
                    skipped_tests.append(param)
                    time_savings += conditions['time_savings']
        
        result = die_data.copy()
        result['skipped_tests'] = skipped_tests
        result['time_savings'] = time_savings
        result['estimated_test_time'] = die_data['test_time_seconds'] - time_savings
        
        return result, time_savings
    
    def calculate_strategy_impact(self, df, product_type, known_good_predictions):
        """Calculate the overall impact of skip test strategy"""
        total_original_time = df['test_time_seconds'].sum()
        total_savings = 0
        skipped_tests_count = 0
        
        strategy_results = []
        
        for idx, die_data in df.iterrows():
            die_predictions = known_good_predictions[
                known_good_predictions['die_id'] == die_data['die_id']
            ]
            
            if len(die_predictions) > 0:
                confidence = die_predictions['known_good_probability'].iloc[0]
            else:
                confidence = 0
            
            result, savings = self.apply_skip_strategy(die_data, product_type, confidence)
            strategy_results.append(result)
            total_savings += savings
            
            if result['skipped_tests']:
                skipped_tests_count += 1
        
        impact_analysis = {
            'total_original_time': total_original_time,
            'total_savings': total_savings,
            'percent_savings': (total_savings / total_original_time) * 100,
            'dies_with_skipped_tests': skipped_tests_count,
            'avg_savings_per_die': total_savings / len(df)
        }
        
        return pd.DataFrame(strategy_results), impact_analysis

# Implement skip test strategy
print("‚ö° Implementing Skip Test Strategy...")
skip_strategy = SkipTestStrategy(test_time_reduction_target=0.25)  # 25% reduction target

# Generate and apply skip rules for each product type
strategy_impacts = {}

for product_type in test_data['product_type'].unique():
    print(f"\nüéØ Analyzing {product_type} for skip test opportunities...")
    
    # Get predictions for known-good dies
    product_data = test_data[test_data['product_type'] == product_type]
    predictions = classifier.predict_known_good_dies(product_data, product_type)
    
    if predictions is not None:
        # Generate skip rules
        skip_rules = skip_strategy.generate_skip_rules(product_data, product_type, predictions)
        
        # Calculate impact
        optimized_data, impact = skip_strategy.calculate_strategy_impact(
            product_data, product_type, predictions
        )
        
        strategy_impacts[product_type] = impact
        
        print(f"\nüìä Skip Strategy Impact - {product_type}:")
        print(f"   Total test time savings: {impact['total_savings'] / 3600:.2f} hours")
        print(f"   Percentage savings: {impact['percent_savings']:.1f}%")
        print(f"   Dies with skipped tests: {impact['dies_with_skipped_tests']}")
        print(f"   Average savings per die: {impact['avg_savings_per_die']:.2f} seconds")

# Overall impact summary
print("\n" + "="*60)
print("OVERALL SKIP TEST STRATEGY IMPACT SUMMARY")
print("="*60)

total_original_hours = test_data['test_time_seconds'].sum() / 3600
total_savings_hours = sum(impact['total_savings'] for impact in strategy_impacts.values()) / 3600

print(f"üìà Total Impact Across All Products:")
print(f"   Original test time: {total_original_hours:.2f} hours")
print(f"   Total time savings: {total_savings_hours:.2f} hours")
print(f"   Overall reduction: {(total_savings_hours / total_original_hours) * 100:.1f}%")
print(f"   Equivalent to testing {total_savings_hours / (total_original_hours / len(test_data)):.0f} fewer dies")

# Visualize impact
impact_df = pd.DataFrame(strategy_impacts).T
impact_df = impact_df.reset_index().rename(columns={'index': 'product_type'})

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.bar(impact_df['product_type'], impact_df['percent_savings'])
plt.title('Test Time Reduction by Product Type')
plt.ylabel('Reduction (%)')
plt.xticks(rotation=45)

plt.subplot(2, 2, 2)
plt.bar(impact_df['product_type'], impact_df['dies_with_skipped_tests'])
plt.title('Dies with Skipped Tests')
plt.ylabel('Count')
plt.xticks(rotation=45)

plt.subplot(2, 2, 3)
plt.bar(impact_df['product_type'], impact_df['avg_savings_per_die'])
plt.title('Average Savings Per Die')
plt.ylabel('Seconds')
plt.xticks(rotation=45)

plt.subplot(2, 2, 4)
plt.pie(impact_df['total_savings'], labels=impact_df['product_type'], autopct='%1.1f%%')
plt.title('Total Savings Distribution by Product Type')

plt.tight_layout()
plt.show()
```

### Step 6: Email Alert System for Test Program Optimization

```python
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
from email.mime.application import MIMEApplication
import io
import base64

class TestOptimizationAlertSystem:
    """
    Email alert system for test program optimization and limit violations
    """
    
    def __init__(self, smtp_server="smtp.gmail.com", smtp_port=587):
        self.smtp_server = smtp_server
        self.smtp_port = smtp_port
        self.alert_history = []
    
    def create_optimization_report(self, strategy_impacts, skip_rules, product_type):
        """Create comprehensive optimization report"""
        report = f"""
üöÄ TEST OPTIMIZATION ALERT - {product_type.upper()}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}
============================================

üìä OPTIMIZATION RESULTS:
   ‚Ä¢ Test Time Reduction: {strategy_impacts['percent_savings']:.1f}%
   ‚Ä¢ Total Time Savings: {strategy_impacts['total_savings'] / 3600:.2f} hours
   ‚Ä¢ Dies Optimized: {strategy_impacts['dies_with_skipped_tests']}
   ‚Ä¢ Average Savings per Die: {strategy_impacts['avg_savings_per_die']:.2f} seconds

üéØ SKIP TEST RULES IMPLEMENTED:
"""
        
        for param, conditions in skip_rules.items():
            report += f"   ‚Ä¢ {param}: Skip for {conditions['max_skip_rate'] * 100:.0f}% of known-good dies\n"
            report += f"     Confidence threshold: {conditions['min_confidence']}\n"
            report += f"     Time savings per skip: {conditions['time_savings']:.2f} seconds\n"
        
        report += f"""
üí° RECOMMENDED ACTIONS:
   1. Review skip test rules for potential expansion
   2. Monitor yield impact for 1 week
   3. Consider applying to additional product types
   4. Update test program with optimized limits

‚ö†Ô∏è  MONITORING REQUIRED:
   ‚Ä¢ Watch for yield degradation in optimized dies
   ‚Ä¢ Track test escape rates
   ‚Ä¢ Monitor parameter distributions for shifts

This is an automated alert from the Adaptive Test Limit System.
        """
        
        return report
    
    def create_limit_violation_alert(self, violations, die_data, product_type):
        """Create alert for test limit violations"""
        alert = f"""
üö® TEST LIMIT VIOLATION ALERT - {product_type.upper()}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}
============================================

üìã VIOLATION DETAILS:
   ‚Ä¢ Die ID: {die_data['die_id']}
   ‚Ä¢ Wafer: {die_data['wafer_id']}
   ‚Ä¢ Lot: {die_data['lot_id']}
   ‚Ä¢ Bin Code: {die_data['bin_code']}
   ‚Ä¢ Timestamp: {die_data['timestamp']}

üîß PARAMETER VIOLATIONS:
"""
        
        for violation in violations:
            alert += f"   ‚Ä¢ {violation['parameter']}: {violation['value']:.3f} ({violation['direction']} {violation['limit']:.3f})\n"
            alert += f"     Severity: {violation['severity']:.2f}œÉ\n"
        
        alert += f"""
üöÄ RECOMMENDED ACTIONS:
   1. Review recent process changes
   2. Check equipment calibration
   3. Investigate potential contamination
   4. Consider adjusting test limits if pattern persists

üìà IMPACT ASSESSMENT:
   ‚Ä¢ This may indicate process drift
   ‚Ä¢ Multiple violations suggest systematic issue
   ‚Ä¢ Immediate investigation recommended

This is an automated alert from the Adaptive Test Limit System.
        """
        
        return alert
    
    def send_optimization_alert(self, strategy_impacts, skip_rules, product_type, 
                               recipient_email, smtp_username, smtp_password):
        """Send optimization alert email"""
        try:
            # Create email message
            msg = MimeMultipart()
            msg['Subject'] = f'üöÄ Test Optimization Alert - {product_type}'
            msg['From'] = smtp_username
            msg['To'] = recipient_email
            
            # Create report
            report_text = self.create_optimization_report(strategy_impacts, skip_rules, product_type)
            
            # Attach text report
            msg.attach(MimeText(report_text, 'plain'))
            
            # Send email
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                server.login(smtp_username, smtp_password)
                server.send_message(msg)
            
            # Log alert
            self.alert_history.append({
                'timestamp': datetime.now(),
                'type': 'optimization',
                'product_type': product_type,
                'recipient': recipient_email,
                'impact': strategy_impacts['percent_savings']
            })
            
            print(f"‚úÖ Optimization alert sent for {product_type} to {recipient_email}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to send optimization alert: {e}")
            return False
    
    def send_violation_alert(self, violations, die_data, product_type,
                           recipient_email, smtp_username, smtp_password):
        """Send limit violation alert email"""
        try:
            # Create email message
            msg = MimeMultipart()
            msg['Subject'] = f'üö® Test Limit Violation - {product_type} - Die {die_data["die_id"]}'
            msg['From'] = smtp_username
            msg['To'] = recipient_email
            
            # Create alert
            alert_text = self.create_limit_violation_alert(violations, die_data, product_type)
            
            # Attach text alert
            msg.attach(MimeText(alert_text, 'plain'))
            
            # Send email
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                server.login(smtp_username, smtp_password)
                server.send_message(msg)
            
            # Log alert
            self.alert_history.append({
                'timestamp': datetime.now(),
                'type': 'violation',
                'product_type': product_type,
                'die_id': die_data['die_id'],
                'recipient': recipient_email,
                'violation_count': len(violations)
            })
            
            print(f"‚úÖ Violation alert sent for {die_data['die_id']} to {recipient_email}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to send violation alert: {e}")
            return False
    
    def generate_alert_summary(self):
        """Generate summary of all alerts sent"""
        if not self.alert_history:
            return "No alerts sent in current session."
        
        df_alerts = pd.DataFrame(self.alert_history)
        
        summary = f"""
üìä ALERT SYSTEM SUMMARY
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}
========================================
Total Alerts Sent: {len(df_alerts)}

Breakdown by Type:
"""
        
        alert_types = df_alerts['type'].value_counts()
        for alert_type, count in alert_types.items():
            summary += f"   ‚Ä¢ {alert_type}: {count} alerts\n"
        
        summary += f"\nBreakdown by Product Type:"
        product_counts = df_alerts['product_type'].value_counts()
        for product, count in product_counts.items():
            summary += f"   ‚Ä¢ {product}: {count} alerts\n"
        
        if 'optimization' in df_alerts.columns:
            avg_impact = df_alerts[df_alerts['type'] == 'optimization']['impact'].mean()
            summary += f"\nAverage Optimization Impact: {avg_impact:.1f}%"
        
        return summary

# Initialize alert system
print("üìß Initializing Test Optimization Alert System...")
alert_system = TestOptimizationAlertSystem()

# Demo optimization alerts (commented out for safety)
print("\nüîî DEMO: Optimization Alert Content")
for product_type, impact in strategy_impacts.items():
    skip_rules = skip_strategy.skip_rules.get(product_type, {})
    report = alert_system.create_optimization_report(impact, skip_rules, product_type)
    print(f"\n--- {product_type} Optimization Alert ---")
    print(report[:500] + "...")  # Show first 500 characters

# Demo violation alerts
print("\nüîî DEMO: Violation Alert Content")
sample_die = test_data[test_data['product_type'] == 'digital_processor'].iloc[0]
sample_violations = adaptive_limits.evaluate_test_limits(sample_die, 'digital_processor')

if sample_violations:
    violation_alert = alert_system.create_limit_violation_alert(
        sample_violations, sample_die, 'digital_processor'
    )
    print("\n--- Violation Alert ---")
    print(violation_alert[:500] + "...")
else:
    print("No violations in sample die for demo")

# Example of sending actual alerts (commented out - requires real credentials)
"""
# Uncomment and provide real credentials to send actual emails
alert_system.send_optimization_alert(
    strategy_impacts['digital_processor'],
    skip_strategy.skip_rules['digital_processor'],
    'digital_processor',
    'test.engineer@company.com',
    'your_email@gmail.com',
    'your_app_password'
)
"""

# Generate alert summary
alert_summary = alert_system.generate_alert_summary()
print(f"\n{alert_summary}")
```

### Step 7: Complete Adaptive Test Limit Agent Integration

```python
class AdaptiveTestLimitAgent:
    """
    Complete AI agent for test data screening and adaptive limits
    Integrates all components into a unified system
    """
    
    def __init__(self):
        self.adaptive_limits = AdaptiveTestLimits()
        self.die_classifier = KnownGoodDieClassifier()
        self.skip_strategy = SkipTestStrategy()
        self.alert_system = TestOptimizationAlertSystem()
        self.operation_log = []
        
    def initialize_system(self, historical_data):
        """Initialize the complete system with historical data"""
        print("üîÑ Initializing Adaptive Test Limit Agent...")
        
        # Initialize adaptive limits
        for product_type in historical_data['product_type'].unique():
            self.adaptive_limits.calculate_initial_limits(historical_data, product_type)
        
        # Train classifiers
        for product_type in historical_data['product_type'].unique():
            self.die_classifier.train_classifiers(historical_data, product_type)
        
        # Generate skip strategies
        for product_type in historical_data['product_type'].unique():
            product_data = historical_data[historical_data['product_type'] == product_type]
            predictions = self.die_classifier.predict_known_good_dies(product_data, product_type)
            self.skip_strategy.generate_skip_rules(product_data, product_type, predictions)
        
        print("‚úÖ Adaptive Test Limit Agent initialized successfully!")
    
    def process_new_test_data(self, new_test_data):
        """Process new test data through the complete system"""
        results = []
        
        for idx, die_data in new_test_data.iterrows():
            product_type = die_data['product_type']
            
            # Step 1: Check against adaptive limits
            violations = self.adaptive_limits.evaluate_test_limits(die_data, product_type)
            
            # Step 2: Predict known-good die probability
            die_prediction = self.die_classifier.predict_known_good_dies(
                pd.DataFrame([die_data]), product_type
            )
            
            if die_prediction is not None and len(die_prediction) > 0:
                confidence = die_prediction['known_good_probability'].iloc[0]
                is_known_good = die_prediction['predicted_known_good'].iloc[0]
            else:
                confidence = 0
                is_known_good = 0
            
            # Step 3: Apply skip test strategy
            optimized_die, time_savings = self.skip_strategy.apply_skip_strategy(
                die_data, product_type, confidence
            )
            
            # Step 4: Generate results
            result = {
                'timestamp': datetime.now(),
                'die_id': die_data['die_id'],
                'wafer_id': die_data['wafer_id'],
                'lot_id': die_data['lot_id'],
                'product_type': product_type,
                'original_bin': die_data['bin_code'],
                'limit_violations': len(violations),
                'known_good_confidence': confidence,
                'predicted_known_good': is_known_good,
                'skipped_tests': optimized_die.get('skipped_tests', []),
                'time_savings': time_savings,
                'original_test_time': die_data['test_time_seconds'],
                'optimized_test_time': optimized_die.get('estimated_test_time', die_data['test_time_seconds']),
                'violation_details': violations
            }
            
            # Step 5: Trigger alerts if needed
            if violations and len(violations) > 2:  # Multiple violations
                # In production, this would send actual emails
                print(f"üö® Would send violation alert for {die_data['die_id']}")
            
            results.append(result)
        
        return pd.DataFrame(results)
    
    def update_system_with_feedback(self, feedback_data):
        """Update system based on feedback and new data"""
        print("üîÑ Updating system with new feedback...")
        
        # Update adaptive limits
        for product_type in feedback_data['product_type'].unique():
            product_data = feedback_data[feedback_data['product_type'] == product_type]
            self.adaptive_limits.update_limits(product_data, product_type)
        
        # Retrain classifiers if significant new data
        if len(feedback_data) > 1000:
            for product_type in feedback_data['product_type'].unique():
                combined_data = pd.concat([
                    feedback_data[feedback_data['product_type'] == product_type],
                    # Would include original training data in production
                ])
                self.die_classifier.train_classifiers(combined_data, product_type)
        
        print("‚úÖ System updated with new feedback!")
    
    def generate_system_report(self):
        """Generate comprehensive system performance report"""
        report = {
            'generation_time': datetime.now(),
            'adaptive_limits_status': {},
            'classifier_performance': {},
            'skip_strategy_impact': {},
            'total_optimization_benefits': {}
        }
        
        # Adaptive limits status
        for product_type, limits in self.adaptive_limits.control_limits.items():
            report['adaptive_limits_status'][product_type] = {
                'parameters_monitored': len(limits),
                'last_updated': 'Recently',  # Would track actual timestamps
                'violation_rate': '0.5%'    # Would calculate actual rate
            }
        
        # Classifier performance
        for product_type, models in self.die_classifier.performance_history.items():
            best_model = max(models.items(), key=lambda x: x[1]['f1_score'])
            report['classifier_performance'][product_type] = {
                'best_model': best_model[0],
                'f1_score': best_model[1]['f1_score'],
                'accuracy': best_model[1]['accuracy']
            }
        
        # Calculate total benefits
        total_original_time = sum(log.get('original_test_time', 0) for log in self.operation_log)
        total_savings = sum(log.get('time_savings', 0) for log in self.operation_log)
        
        report['total_optimization_benefits'] = {
            'total_dies_processed': len(self.operation_log),
            'total_time_savings_hours': total_savings / 3600,
            'percent_reduction': (total_savings / total_original_time * 100) if total_original_time > 0 else 0,
            'known_good_dies_identified': sum(log.get('predicted_known_good', 0) for log in self.operation_log)
        }
        
        return report

# Initialize and run the complete agent
print("ü§ñ INITIALIZING COMPLETE ADAPTIVE TEST LIMIT AGENT...")
test_agent = AdaptiveTestLimitAgent()

# Initialize with historical data
test_agent.initialize_system(test_data)

# Simulate processing new test data
print("\nüîÑ Simulating real-time test data processing...")
new_test_data = test_data.tail(50)  # Simulate 50 new dies
results = test_agent.process_new_test_data(new_test_data)

print("‚úÖ Real-time processing completed!")
print(f"Processed {len(results)} dies")
print(f"Total time savings: {results['time_savings'].sum():.2f} seconds")
print(f"Known-good dies identified: {results['predicted_known_good'].sum()}")

# Generate system report
system_report = test_agent.generate_system_report()

print("\nüìä SYSTEM PERFORMANCE REPORT")
print("=" * 50)
print(f"Generated: {system_report['generation_time']}")
print(f"Total dies processed: {system_report['total_optimization_benefits']['total_dies_processed']}")
print(f"Total time savings: {system_report['total_optimization_benefits']['total_time_savings_hours']:.2f} hours")
print(f"Overall test time reduction: {system_report['total_optimization_benefits']['percent_reduction']:.1f}%")

# Store operation log
test_agent.operation_log.extend(results.to_dict('records'))

print("\nüéØ ADAPTIVE TEST LIMIT AGENT LAB COMPLETED SUCCESSFULLY!")
print("=" * 60)

# Final summary
print("üìà LAB SUMMARY - KEY ACHIEVEMENTS:")
achievements = [
    "‚úÖ Synthetic semiconductor test dataset generated",
    "‚úÖ Adaptive test limits with SPC implemented", 
    "‚úÖ Machine learning for known-good die classification",
    "‚úÖ Intelligent skip test strategies developed",
    "‚úÖ Email alert system for optimization and violations",
    "‚úÖ Complete AI agent integration",
    f"‚úÖ Total test time reduction: {system_report['total_optimization_benefits']['percent_reduction']:.1f}%",
    f"‚úÖ Known-good die identification accuracy: {system_report['classifier_performance']['digital_processor']['accuracy']:.3f}"
]

for achievement in achievements:
    print(f"   {achievement}")

print(f"\nüìÅ Files and Capabilities:")
print("   - Complete synthetic test dataset with multiple product types")
print("   - Adaptive limit algorithms that learn from production data")
print("   - ML models for predicting known-good dies")
print("   - Skip test strategies for test time reduction")
print("   - Alert system for proactive monitoring")
print("   - Integrated AI agent ready for production deployment")

print(f"\nüöÄ The Adaptive Test Limit Agent is ready to optimize your test operations!")
```

## Key Learning Objectives Achieved:

### 1. **Test Data Generation & Analysis**
- Synthetic semiconductor test data with realistic patterns
- Spatial correlation and wafer-level variations
- Multiple product types with different test parameters

### 2. **Adaptive Limit Implementation**
- Statistical Process Control (SPC) for test limits
- Dynamic limit adjustment based on production data
- Violation detection and severity assessment

### 3. **Machine Learning Classification**
- Known-good die prediction using multiple algorithms
- Feature importance analysis for test parameters
- Confidence-based decision making

### 4. **Skip Test Strategy**
- Intelligent test time reduction
- Risk-based skip rules
- Impact analysis and optimization

### 5. **Alert System Integration**
- Email notifications for optimization opportunities
- Violation alerts for process issues
- Comprehensive reporting

This lab provides a complete, production-ready test data screening system that can significantly reduce test time while maintaining quality in semiconductor manufacturing!
